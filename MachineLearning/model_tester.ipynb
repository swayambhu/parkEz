{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b200f9c1-5ddb-4f0a-9d94-5df8caf23174",
   "metadata": {},
   "source": [
    "##  This notebook takes labels from build_training_data/training_images/labels.json and images from build_training_data/training_images/ and uses that to create parking spot car detection models for ParkEz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6463bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462ee316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layer 1\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)  \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Convolutional layer 2\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Convolutional layer 3\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Convolutional layer 4\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        # Convolutional layer 5\n",
    "        self.conv5 = nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        # Max pool layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(512 * 8 * 8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layer 1\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.pool(out)\n",
    "\n",
    "        # Convolutional layer 2\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.pool(out)\n",
    "\n",
    "        # Convolutional layer 3\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        out = self.pool(out)\n",
    "\n",
    "        # Convolutional layer 4\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.relu4(out)\n",
    "        out = self.pool(out)\n",
    "\n",
    "        # Convolutional layer 5\n",
    "        out = self.conv5(out)\n",
    "        out = self.bn5(out)\n",
    "        out = self.relu5(out)\n",
    "        out = self.pool(out)\n",
    "\n",
    "        # Flatten for fully connected layer\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Fully connected layer 1\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Fully connected layer 2\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Fully connected layer 3\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# Originally in Model_Maker notebook, this preps cropped parking spaces for ML processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ToTensor(),  # Convert to PyTorch tensor\n",
    "    transforms.Normalize((0.5,0.5,0.5,), (0.5,0.5,0.5,))  # Normalize pixel values in the range [-1, 1]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb55ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    old_models = {}\n",
    "    for key in spot_keys:\n",
    "        model_path = os.path.join('Archive', 'old_models', key + '.pth')\n",
    "        old_models[key] = CNN()\n",
    "        old_models[key].load_state_dict(torch.load(model_path)) \n",
    "        old_models[key].eval() \n",
    "    return old_models\n",
    "\n",
    "def predict_image(to_predict_image, predicting_model_dict):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ret = {}\n",
    "    for key in spot_keys:\n",
    "        dir_name = 'input'\n",
    "        full_path = os.path.join(dir_name, to_predict_image)\n",
    "        image = Image.open(full_path)\n",
    "        spots_file_path = os.path.join('spots.json')\n",
    "        with open(spots_file_path, 'r') as spots_file:\n",
    "            spots_data = json.load(spots_file)\n",
    "        x, x_w, y, y_h = spots_data[key]\n",
    "        cropped_image = image.crop((x, y, x_w, y_h))\n",
    "        input_tensor = transform(cropped_image)\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Move the input tensor to the same device as the model\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = predicting_model_dict[key](input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "\n",
    "        prediction = predicted.item()\n",
    "        ret[key] = False\n",
    "        if prediction == 0: ret[key] = True\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46e879a-6bd7-4f01-b61f-145beb6132ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls lot_image_uploader/baseline_ml_diag\n",
    "def predict_image(to_predict_image, spots, predicting_model_dict):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ret = {}\n",
    "    for key in spots:\n",
    "        full_path = os.path.join('lot_image_uploader', 'baseline_ml_diag', to_predict_image)\n",
    "        image = Image.open(full_path)\n",
    "        x, x_w, y, y_h = spots_data[key]\n",
    "        cropped_image = image.crop((x, y, x_w, y_h))\n",
    "        input_tensor = transform(cropped_image)\n",
    "        input_tensor = input_tensor.unsqueeze(0)\n",
    "        \n",
    "        # Move the input tensor to the same device as the model\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = predicting_model_dict[key](input_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "\n",
    "        prediction = predicted.item()\n",
    "        ret[key] = False\n",
    "        if prediction == 0: ret[key] = True\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c4bcc77-9e97-48e1-96ac-b64e767516fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colltown_202310291259.jpg {'b2': True, 'b3': True}\n",
      "colltown_202310290938.jpg {'b2': False, 'b3': True}\n",
      "colltown_202310291446.jpg {'b2': True, 'b3': True}\n",
      "colltown_202310291056.jpg {'b2': True, 'b3': True}\n",
      "colltown_202310291229.jpg {'b2': False, 'b3': False}\n",
      "colltown_202310291158.jpg {'b2': True, 'b3': True}\n",
      "colltown_202310291330.jpg {'b2': True, 'b3': True}\n",
      "colltown_202310291416.jpg {'b2': True, 'b3': True}\n",
      "colltown_202310291128.jpg {'b2': True, 'b3': True}\n",
      "colltown_202310291025.jpg {'b2': False, 'b3': True}\n"
     ]
    }
   ],
   "source": [
    "def get_models():\n",
    "    old_models = {}\n",
    "    for key in ['b2', 'b3']:\n",
    "        model_path = key + '.pth'\n",
    "        old_models[key] = CNN()\n",
    "        # Add map_location='cpu' to load the model onto the CPU\n",
    "        old_models[key].load_state_dict(torch.load(model_path, map_location='cpu')) \n",
    "        old_models[key].eval()\n",
    "    return old_models\n",
    "\n",
    "models_es_dict = get_models()\n",
    "\n",
    "directory_path = 'lot_image_uploader/baseline_ml_diag'\n",
    "spots_file_path = 'build_training_data/spots.json'\n",
    "with open(spots_file_path, 'r') as spots_file:\n",
    "    spots_data = json.load(spots_file)\n",
    "\n",
    "file_names = os.listdir(directory_path)\n",
    "for im in file_names:\n",
    "    print( im + ' ' + str(predict_image(im, ['b2','b3'], models_es_dict)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
